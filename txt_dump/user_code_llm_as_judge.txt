File Name: context.py

-----------------------------

File Content: 

"""Define the runtime context information for the agent."""

import os
from dataclasses import dataclass, field, fields

from typing_extensions import Annotated

from memory_agent import prompts


@dataclass(kw_only=True)
class Context:
    """Main context class for the memory graph system."""

    user_id: str = "default"
    """The ID of the user to remember in the conversation."""

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="anthropic/claude-3-5-sonnet-20240620",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )

    system_prompt: str = prompts.SYSTEM_PROMPT

    def __post_init__(self):
        """Fetch env vars for attributes that were not passed as args."""
        for f in fields(self):
            if not f.init:
                continue

            if getattr(self, f.name) == f.default:
                setattr(self, f.name, os.environ.get(f.name.upper(), f.default))




File Name: graph.py

-----------------------------

File Content: 

"""Graphs that extract memories on a schedule."""

import asyncio
import logging
from datetime import datetime
from typing import cast

from langchain.chat_models import init_chat_model
from langgraph.graph import END, StateGraph
from langgraph.runtime import Runtime
from langgraph.store.base import BaseStore

from memory_agent import tools, utils
from memory_agent.context import Context
from memory_agent.state import State
from memory_agent.tools import MemoryCategory

logger = logging.getLogger(__name__)

# Initialize the language model to be used for memory extraction
llm = init_chat_model()


async def determine_message_categories(messages: list) -> list[MemoryCategory]:
    """Determine the categories relevant to recent messages."""
    recent_content = " ".join(
        m.content for m in messages[-3:] if hasattr(m, 'content') and m.content
    )
    
    # Use the same logic as memory categorization
    category = await tools.determine_memory_category(recent_content, "")
    
    # For message categorization, we might want to return multiple relevant categories
    # For now, return the determined category plus OTHER to catch general memories
    if category == MemoryCategory.PERSONAL:
        return [MemoryCategory.PERSONAL, MemoryCategory.OTHER]
    elif category == MemoryCategory.PROFESSIONAL:
        return [MemoryCategory.PROFESSIONAL, MemoryCategory.OTHER]
    else:
        return [MemoryCategory.OTHER, MemoryCategory.PERSONAL, MemoryCategory.PROFESSIONAL]


async def call_model(state: State, runtime: Runtime[Context]) -> dict:
    """Extract the user's state from the conversation and update the memory."""
    user_id = runtime.context.user_id
    model = runtime.context.model
    system_prompt = runtime.context.system_prompt

    # Check if we're waiting for confirmation
    if state.awaiting_confirmation:
        last_message = state.messages[-1]
        if hasattr(last_message, 'content') and last_message.content:
            if last_message.content.lower().strip() == "accept":
                # User accepted, proceed with storing the memory
                if state.pending_memory:
                    await tools.upsert_memory(
                        content=state.pending_memory["content"],
                        context=state.pending_memory["context"],
                        category=MemoryCategory(state.pending_memory["category"]),
                        user_id=user_id,
                        store=cast(BaseStore, runtime.store),
                    )
                    msg = await llm.ainvoke([
                        {"role": "system", "content": "Confirm that the memory has been saved."},
                        {"role": "user", "content": "Memory saved successfully!"}
                    ])
                    return {
                        "messages": [msg],
                        "pending_memory": None,
                        "awaiting_confirmation": False
                    }
            else:
                # User rejected, clear pending memory
                msg = await llm.ainvoke([
                    {"role": "system", "content": "Acknowledge that the memory was rejected."},
                    {"role": "user", "content": "Memory not saved."}
                ])
                return {
                    "messages": [msg],
                    "pending_memory": None,
                    "awaiting_confirmation": False
                }

    # Determine relevant categories for memory retrieval
    relevant_categories = await determine_message_categories(state.messages)
    
    # Retrieve memories from relevant categories (if store is available)
    all_memories = []
    if runtime.store:
        for category in relevant_categories:
            try:
                memories = await cast(BaseStore, runtime.store).asearch(
                    ("memories", user_id, category.value),
                    query=str([m.content for m in state.messages[-3:]]),
                    limit=4,  # Reduce per category to keep total manageable
                )
                all_memories.extend(memories)
            except Exception as e:
                logger.warning(f"Error retrieving memories for category {category}: {e}")
        
        # Sort by relevance score and take top 10
        all_memories.sort(key=lambda x: x.score if x.score else 0, reverse=True)
        memories = all_memories[:10]
    else:
        memories = []

    # Format memories for inclusion in the prompt
    formatted = "\n".join(
        f"[{mem.key}]: {mem.value} (similarity: {mem.score})" for mem in memories
    )
    if formatted:
        formatted = f"""
<memories>
{formatted}
</memories>"""

    # Prepare the system prompt with user memories and current time
    # This helps the model understand the context and temporal relevance
    sys = system_prompt.format(user_info=formatted, time=datetime.now().isoformat())

    # Invoke the language model with the prepared prompt and tools
    # "bind_tools" gives the LLM the JSON schema for all tools in the list so it knows how
    # to use them.
    msg = await llm.bind_tools([tools.request_memory_confirmation]).ainvoke(
        [{"role": "system", "content": sys}, *state.messages],
        context=utils.split_model_and_provider(model),
    )
    return {"messages": [msg]}


async def request_confirmation(state: State, runtime: Runtime[Context]) -> dict:
    """Handle memory confirmation requests."""
    tool_calls = getattr(state.messages[-1], "tool_calls", [])
    
    if not tool_calls:
        return {"messages": []}
    
    # Process the first memory confirmation request
    tc = tool_calls[0]
    if tc["name"] == "request_memory_confirmation":
        args = tc["args"]
        content = args["content"]
        context = args["context"] 
        category = args["category"]
        
        # Determine category if it's a string
        if isinstance(category, str):
            category = MemoryCategory(category)
        
        # Store pending memory details
        pending_memory = {
            "content": content,
            "context": context,
            "category": category.value
        }
        
        # Create confirmation message
        confirmation_msg = await tools.request_memory_confirmation(content, context, category)
        
        result = {
            "role": "tool",
            "content": confirmation_msg,
            "tool_call_id": tc["id"],
        }
        
        return {
            "messages": [result],
            "pending_memory": pending_memory,
            "awaiting_confirmation": True
        }
    
    return {"messages": []}


async def store_memory(state: State, runtime: Runtime[Context]):
    # This function is kept for backwards compatibility but shouldn't be called
    # in the new workflow since we use confirmation first
    tool_calls = getattr(state.messages[-1], "tool_calls", [])

    # Concurrently execute all upsert_memory calls
    saved_memories = await asyncio.gather(
        *(
            tools.upsert_memory(
                **tc["args"],
                user_id=runtime.context.user_id,
                store=cast(BaseStore, runtime.store),
            )
            for tc in tool_calls
        )
    )

    # Format the results of memory storage operations
    # This provides confirmation to the model that the actions it took were completed
    results = [
        {
            "role": "tool",
            "content": mem,
            "tool_call_id": tc["id"],
        }
        for tc, mem in zip(tool_calls, saved_memories)
    ]
    return {"messages": results}


def route_message(state: State):
    """Determine the next step based on the presence of tool calls."""
    msg = state.messages[-1]
    if getattr(msg, "tool_calls", None):
        # Check if it's a memory confirmation request
        tool_calls = msg.tool_calls
        if tool_calls and tool_calls[0]["name"] == "request_memory_confirmation":
            return "request_confirmation"
        # Otherwise, handle as normal tool call (backwards compatibility)
        return "store_memory"
    # Otherwise, finish; user can send the next message
    return END


# Create the graph + all nodes
builder = StateGraph(State, context_schema=Context)

# Define the flow of the memory extraction process
builder.add_node(call_model)
builder.add_edge("__start__", "call_model")
builder.add_node(request_confirmation)
builder.add_node(store_memory)
builder.add_conditional_edges("call_model", route_message, ["request_confirmation", "store_memory", END])
# After requesting confirmation, return control to user
builder.add_edge("request_confirmation", END)
# After storing memory (backwards compatibility), return to model
builder.add_edge("store_memory", "call_model")
graph = builder.compile()
graph.name = "MemoryAgent"


__all__ = ["graph"]




File Name: prompts.py

-----------------------------

File Content: 

"""Define default prompts."""

SYSTEM_PROMPT = """You are a helpful and friendly chatbot with an enhanced memory system. Get to know the user! \
Ask questions! Be spontaneous! 

IMPORTANT: When you want to remember something about the user, use the request_memory_confirmation tool. 
This will automatically categorize the memory as personal, professional, or other, and ask the user for 
confirmation before saving. Only memories that users confirm with 'accept' will be saved.

Your memory system organizes information into categories:
- PERSONAL: Family, friends, hobbies, health, personal preferences
- PROFESSIONAL: Work, career, business, skills, projects  
- OTHER: General facts, weather, neutral observations

{user_info}

System Time: {time}"""




File Name: state.py

-----------------------------

File Content: 

"""Define the shared values."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Any, Optional

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import Annotated


@dataclass(kw_only=True)
class State:
    """Main graph state."""

    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    
    pending_memory: Optional[Dict[str, Any]] = None
    """Memory awaiting user confirmation."""
    
    awaiting_confirmation: bool = False
    """Whether the system is waiting for user confirmation."""


__all__ = [
    "State",
]




File Name: tools.py

-----------------------------

File Content: 

"""Define he agent's tools."""

import uuid
from enum import Enum
from typing import Annotated, Optional

from langchain_core.tools import InjectedToolArg
from langgraph.store.base import BaseStore


class MemoryCategory(str, Enum):
    """Memory categories for organizing user memories."""
    PERSONAL = "personal"
    PROFESSIONAL = "professional"
    OTHER = "other"


async def determine_memory_category(
    content: str,
    context: str,
) -> MemoryCategory:
    """Determine the category of a memory based on its content and context.
    
    This function categorizes memories into personal, professional, or other
    based on keywords and context clues.
    
    Args:
        content: The main content of the memory
        context: Additional context for the memory
        
    Returns:
        MemoryCategory: The determined category
    """
    combined_text = f"{content} {context}".lower()
    
    # Keywords for professional category
    professional_keywords = [
        "work", "job", "career", "office", "colleague", "boss", "manager",
        "meeting", "project", "deadline", "client", "business", "company",
        "salary", "promotion", "team", "department", "skills", "interview",
        "resume", "professional", "workplace", "corporate", "industry"
    ]
    
    # Keywords for personal category
    personal_keywords = [
        "family", "friend", "hobby", "vacation", "birthday", "anniversary",
        "relationship", "spouse", "partner", "children", "kids", "parent",
        "home", "house", "personal", "health", "doctor", "medical", "food",
        "restaurant", "movie", "book", "music", "travel", "pet", "dog", "cat"
    ]
    
    professional_score = sum(1 for keyword in professional_keywords if keyword in combined_text)
    personal_score = sum(1 for keyword in personal_keywords if keyword in combined_text)
    
    if professional_score > personal_score:
        return MemoryCategory.PROFESSIONAL
    elif personal_score > professional_score:
        return MemoryCategory.PERSONAL
    else:
        return MemoryCategory.OTHER


async def request_memory_confirmation(
    content: str,
    context: str,
    category: MemoryCategory,
) -> str:
    """Request user confirmation before saving a memory.
    
    This tool should be called before saving any memory to get user approval.
    
    Args:
        content: The main content of the memory
        context: Additional context for the memory
        category: The determined category for this memory
        
    Returns:
        A message requesting user confirmation
    """
    return f"""I want to save this memory (Category: {category.value}):
Content: {content}
Context: {context}

Please respond with 'accept' to save this memory, or anything else to reject it."""


async def upsert_memory(
    content: str,
    context: str,
    *,
    category: Optional[MemoryCategory] = None,
    memory_id: Optional[uuid.UUID] = None,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Upsert a memory in the database with category-based organization.

    If a memory conflicts with an existing one, then just UPDATE the
    existing one by passing in memory_id - don't create two memories
    that are the same. If the user corrects a memory, UPDATE it.

    Args:
        content: The main content of the memory. For example:
            "User expressed interest in learning about French."
        context: Additional context for the memory. For example:
            "This was mentioned while discussing career options in Europe."
        category: The category of the memory (personal, professional, other).
            If not provided, it will be automatically determined.
        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.
        The memory to overwrite.
    """
    # Determine category if not provided
    if category is None:
        category = await determine_memory_category(content, context)
    
    mem_id = memory_id or uuid.uuid4()
    
    # Store memory with category-based namespace
    namespace = ("memories", user_id, category.value)
    await store.aput(
        namespace,
        key=str(mem_id),
        value={"content": content, "context": context, "category": category.value},
    )
    return f"Stored {category.value} memory {mem_id}"




File Name: utils.py

-----------------------------

File Content: 

"""Utility functions used in our graph."""


def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


