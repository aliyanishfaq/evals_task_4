File Name: context.py

-----------------------------

File Content: 

"""Define the runtime context information for the agent."""

import os
from dataclasses import dataclass, field, fields

from typing_extensions import Annotated

from memory_agent import prompts


@dataclass(kw_only=True)
class Context:
    """Main context class for the memory graph system."""

    user_id: str = "default"
    """The ID of the user to remember in the conversation."""

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="anthropic/claude-3-5-sonnet-20240620",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )

    system_prompt: str = prompts.SYSTEM_PROMPT

    def __post_init__(self):
        """Fetch env vars for attributes that were not passed as args."""
        for f in fields(self):
            if not f.init:
                continue

            if getattr(self, f.name) == f.default:
                setattr(self, f.name, os.environ.get(f.name.upper(), f.default))




File Name: graph.py

-----------------------------

File Content: 

"""Graphs that extract memories on a schedule."""

import asyncio
import logging
from datetime import datetime
from typing import cast
import re

from langchain.chat_models import init_chat_model
from langgraph.graph import END, StateGraph
from langgraph.runtime import Runtime
from langgraph.store.base import BaseStore

from memory_agent import tools, utils
from memory_agent.context import Context
from memory_agent.state import State

logger = logging.getLogger(__name__)

# Initialize the language model to be used for memory extraction
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")


def classify_conversation_category(messages: list) -> list[str]:
    """Classify the conversation to determine which memory categories to search."""
    recent_content = " ".join([
        getattr(m, 'content', '') for m in messages[-3:] 
        if hasattr(m, 'content') and getattr(m, 'content', None)
    ])
    
    categories = []
    
    # Personal keywords
    personal_keywords = [
        'family', 'wife', 'husband', 'child', 'parent', 'sibling', 'friend', 'hobby', 
        'personal', 'home', 'vacation', 'travel', 'health', 'relationship', 'love',
        'like', 'prefer', 'enjoy', 'favorite', 'birthday', 'anniversary'
    ]
    
    # Professional keywords
    professional_keywords = [
        'work', 'job', 'career', 'office', 'manager', 'colleague', 'client', 'project',
        'meeting', 'business', 'company', 'skill', 'certification', 'degree', 'education',
        'interview', 'salary', 'promotion', 'professional'
    ]
    
    content_lower = recent_content.lower()
    
    if any(keyword in content_lower for keyword in personal_keywords):
        categories.append("personal")
    
    if any(keyword in content_lower for keyword in professional_keywords):
        categories.append("professional")
    
    # If no specific category detected or general conversation, include other
    if not categories or len(categories) == 2:
        categories.append("other")
    
    return categories if categories else ["personal", "professional", "other"]


async def call_model(state: State, runtime: Runtime[Context]) -> dict:
    """Extract the user's state from the conversation and update the memory."""
    user_id = runtime.context.user_id
    model = runtime.context.model
    system_prompt = runtime.context.system_prompt

    # Check if we're awaiting user confirmation
    if state.awaiting_confirmation and state.pending_memory:
        last_message_content = getattr(state.messages[-1], 'content', '').lower().strip()
        
        if last_message_content == "accept":
            # User confirmed, save the memory
            mem_data = state.pending_memory
            await tools.classify_and_store_memory(
                content=mem_data["content"],
                context=mem_data["context"], 
                category=mem_data["category"],
                user_id=user_id,
                store=cast(BaseStore, runtime.store),
            )
            response_msg = f"Great! I've saved that {mem_data['category']} memory."
            return {
                "messages": [{"role": "assistant", "content": response_msg}],
                "pending_memory": None,
                "awaiting_confirmation": False
            }
        else:
            # User declined, don't save
            response_msg = "Okay, I won't save that information."
            return {
                "messages": [{"role": "assistant", "content": response_msg}],
                "pending_memory": None,
                "awaiting_confirmation": False
            }

    # Determine conversation category for memory retrieval
    categories_to_search = classify_conversation_category(state.messages)
    
    # Retrieve relevant memories based on conversation category
    all_memories = []
    query_content = " ".join([
        getattr(m, 'content', '') for m in state.messages[-3:] 
        if hasattr(m, 'content') and getattr(m, 'content', None)
    ])
    
    for category in categories_to_search:
        try:
            memories = await cast(BaseStore, runtime.store).asearch(
                ("memories", user_id, category),
                query=query_content,
                limit=5,
            )
            all_memories.extend(memories)
        except Exception:
            # Fallback to old format if new format doesn't exist yet
            memories = await cast(BaseStore, runtime.store).asearch(
                ("memories", user_id),
                query=query_content,
                limit=5,
            )
            all_memories.extend(memories)
            break

    # Sort by relevance and limit
    all_memories.sort(key=lambda x: x.score if x.score else 0, reverse=True)
    all_memories = all_memories[:10]

    # Format memories for inclusion in the prompt
    formatted = "\n".join(
        f"[{mem.key}]: {mem.value} (similarity: {mem.score})" for mem in all_memories
    )
    if formatted:
        formatted = f"""
<memories>
{formatted}
</memories>"""

    # Prepare the system prompt with user memories and current time
    sys = system_prompt.format(user_info=formatted, time=datetime.now().isoformat())

    # Invoke the language model with the new categorized memory tool
    msg = await llm.bind_tools([tools.classify_and_store_memory]).ainvoke(
        [{"role": "system", "content": sys}, *state.messages],
    )
    return {"messages": [msg]}


async def request_confirmation(state: State, runtime: Runtime[Context]):
    """Instead of storing memory immediately, request user confirmation."""
    # Extract tool calls from the last message
    tool_calls = getattr(state.messages[-1], "tool_calls", [])
    
    if not tool_calls:
        return {"messages": []}
    
    # For now, handle only the first memory request
    tc = tool_calls[0]
    args = tc["args"]
    
    # Store the pending memory in state
    pending_memory = {
        "content": args["content"],
        "context": args["context"], 
        "category": args["category"],
        "tool_call_id": tc["id"]
    }
    
    # Create confirmation message
    confirmation_msg = f"I'd like to remember that {args['content']}. Should I save this? (respond with 'accept' to save)"
    
    # Return mock tool result and confirmation request
    tool_result = {
        "role": "tool",
        "content": "Memory prepared for user confirmation",
        "tool_call_id": tc["id"],
    }
    
    assistant_msg = {
        "role": "assistant", 
        "content": confirmation_msg
    }
    
    return {
        "messages": [tool_result, assistant_msg],
        "pending_memory": pending_memory,
        "awaiting_confirmation": True
    }


# Keep the old store_memory for backward compatibility
async def store_memory(state: State, runtime: Runtime[Context]):
    # Extract tool calls from the last message
    tool_calls = getattr(state.messages[-1], "tool_calls", [])

    # Concurrently execute all classify_and_store_memory calls  
    saved_memories = await asyncio.gather(
        *(
            tools.classify_and_store_memory(
                **tc["args"],
                user_id=runtime.context.user_id,
                store=cast(BaseStore, runtime.store),
            )
            for tc in tool_calls
        )
    )

    # Format the results of memory storage operations
    results = [
        {
            "role": "tool",
            "content": mem,
            "tool_call_id": tc["id"],
        }
        for tc, mem in zip(tool_calls, saved_memories)
    ]
    return {"messages": results}


def route_message(state: State):
    """Determine the next step based on the presence of tool calls and state."""
    msg = state.messages[-1]
    if getattr(msg, "tool_calls", None):
        # If there are tool calls, request confirmation first
        return "request_confirmation"
    # Otherwise, finish; user can send the next message
    return END


# Create the graph + all nodes
builder = StateGraph(State, context_schema=Context)

# Define the flow of the memory extraction process
builder.add_node(call_model)
builder.add_edge("__start__", "call_model")

# Add the new confirmation request node
builder.add_node(request_confirmation)
builder.add_conditional_edges("call_model", route_message, ["request_confirmation", END])

# After requesting confirmation, return control to user
builder.add_edge("request_confirmation", END)

graph = builder.compile()
graph.name = "MemoryAgent"

app = graph


__all__ = ["graph"]




File Name: prompts.py

-----------------------------

File Content: 

"""Define default prompts."""

SYSTEM_PROMPT = """You are a helpful and friendly chatbot. Get to know the user! \
Ask questions! Be spontaneous! 

When you identify information that should be remembered about the user:
1. First classify it as "personal" (family, hobbies, preferences), "professional" (work, career, skills), or "other" (general knowledge, casual facts)
2. Before saving ANY memory, you MUST ask the user: "I'd like to remember that [brief summary of the memory]. Should I save this? (respond with 'accept' to save)"
3. Only call the classify_and_store_memory tool after the user confirms with "accept"
4. If they respond with anything else, do not save the memory

When retrieving memories to provide context, determine the category of the current conversation:
- If discussing personal life, family, hobbies → search "personal" memories
- If discussing work, career, professional topics → search "professional" memories  
- If discussing general topics → search "other" memories
- If unsure, search all categories

{user_info}

System Time: {time}"""

MEMORY_CLASSIFICATION_PROMPT = """Based on the conversation context, classify what category this memory belongs to:

PERSONAL: Family, relationships, personal hobbies, personal preferences, health, personal experiences, personal goals
PROFESSIONAL: Work, career, job, business, professional skills, work projects, professional networking, education, certifications
OTHER: General knowledge, casual conversations, facts, opinions on general topics, entertainment preferences

Content: {content}
Context: {context}

Category (personal/professional/other):"""




File Name: state.py

-----------------------------

File Content: 

"""Define the shared values."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Dict, Any

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import Annotated


@dataclass(kw_only=True)
class State:
    """Main graph state."""

    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    
    pending_memory: Optional[Dict[str, Any]] = None
    """Memory awaiting user confirmation before saving."""
    
    awaiting_confirmation: bool = False
    """Whether we're waiting for user confirmation to save a memory."""


__all__ = [
    "State",
]




File Name: tools.py

-----------------------------

File Content: 

"""Define he agent's tools."""

import uuid
from typing import Annotated, Optional, Literal

from langchain_core.tools import InjectedToolArg
from langgraph.store.base import BaseStore


async def classify_and_store_memory(
    content: str,
    context: str,
    category: Literal["personal", "professional", "other"],
    *,
    memory_id: Optional[uuid.UUID] = None,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Classify and store a memory in the database with category.

    First determines if this memory should be saved (requires user confirmation).
    Then stores it in the appropriate category-based namespace.

    Args:
        content: The main content of the memory. For example:
            "User expressed interest in learning about French."
        context: Additional context for the memory. For example:
            "This was mentioned while discussing career options in Europe."
        category: The category of the memory - "personal", "professional", or "other".
            - personal: Personal life, family, hobbies, personal preferences
            - professional: Work, career, business, professional skills
            - other: General knowledge, casual conversations, miscellaneous
        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.
    """
    # This function will now be called only after user confirmation
    mem_id = memory_id or uuid.uuid4()
    await store.aput(
        ("memories", user_id, category),
        key=str(mem_id),
        value={"content": content, "context": context, "category": category},
    )
    return f"Stored {category} memory {mem_id}"


# Keep the old function for backward compatibility
async def upsert_memory(
    content: str,
    context: str,
    *,
    memory_id: Optional[uuid.UUID] = None,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Upsert a memory in the database.

    If a memory conflicts with an existing one, then just UPDATE the
    existing one by passing in memory_id - don't create two memories
    that are the same. If the user corrects a memory, UPDATE it.

    Args:
        content: The main content of the memory. For example:
            "User expressed interest in learning about French."
        context: Additional context for the memory. For example:
            "This was mentioned while discussing career options in Europe."
        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.
        The memory to overwrite.
    """
    mem_id = memory_id or uuid.uuid4()
    await store.aput(
        ("memories", user_id),
        key=str(mem_id),
        value={"content": content, "context": context},
    )
    return f"Stored memory {mem_id}"


async def retrieve_memories_by_category(
    query: str,
    categories: list[Literal["personal", "professional", "other"]],
    *,
    limit: int = 10,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Retrieve memories from specific categories based on a query.

    Args:
        query: The search query to find relevant memories.
        categories: List of categories to search in (personal, professional, other).
        limit: Maximum number of memories to retrieve per category.
    """
    all_memories = []
    
    for category in categories:
        memories = await store.asearch(
            ("memories", user_id, category),
            query=query,
            limit=limit,
        )
        all_memories.extend(memories)
    
    # Sort by relevance score
    all_memories.sort(key=lambda x: x.score if x.score else 0, reverse=True)
    
    return all_memories[:limit]




File Name: utils.py

-----------------------------

File Content: 

"""Utility functions used in our graph."""


def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


