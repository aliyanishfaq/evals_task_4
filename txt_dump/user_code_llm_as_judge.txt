File Name: context.py

-----------------------------

File Content: 

"""Define the runtime context information for the agent."""

import os
from dataclasses import dataclass, field, fields

from typing_extensions import Annotated

from . import prompts


@dataclass(kw_only=True)
class Context:
    """Main context class for the memory graph system."""

    user_id: str = "default"
    """The ID of the user to remember in the conversation."""

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="anthropic/claude-3-5-sonnet-20240620",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )

    system_prompt: str = prompts.SYSTEM_PROMPT

    def __post_init__(self):
        """Fetch env vars for attributes that were not passed as args."""
        for f in fields(self):
            if not f.init:
                continue

            if getattr(self, f.name) == f.default:
                setattr(self, f.name, os.environ.get(f.name.upper(), f.default))




File Name: graph.py

-----------------------------

File Content: 

"""Graphs that extract memories on a schedule."""

import asyncio
import logging
from datetime import datetime
from typing import cast

from langchain.chat_models import init_chat_model
from langgraph.graph import END, StateGraph
from langgraph.runtime import Runtime
from langgraph.store.base import BaseStore

from . import tools, utils
from .context import Context
from .state import State
from .tools import classify_message_category

logger = logging.getLogger(__name__)

# Initialize the language model to be used for memory extraction
llm = init_chat_model()


async def call_model(state: State, runtime: Runtime[Context]) -> dict:
    """Extract the user's state from the conversation and update the memory."""
    user_id = runtime.context.user_id
    model = runtime.context.model
    system_prompt = runtime.context.system_prompt

    # Classify the message category to retrieve relevant memories
    predicted_category = await classify_message_category(state.messages, llm)
    
    # Try to retrieve memories from the predicted category first
    memories = await cast(BaseStore, runtime.store).asearch(
        ("memories", user_id, predicted_category.value),
        query=str([m.content for m in state.messages[-3:]]),
        limit=10,
    )
    
    # If no memories found in predicted category, try other categories  
    if not memories:
        for category in ["personal", "professional", "other"]:
            if category != predicted_category.value:
                memories = await cast(BaseStore, runtime.store).asearch(
                    ("memories", user_id, category),
                    query=str([m.content for m in state.messages[-3:]]),
                    limit=5,
                )
                if memories:
                    break

    # Format memories for inclusion in the prompt
    formatted = "\n".join(
        f"[{mem.key}]: {mem.value} (similarity: {mem.score})" for mem in memories
    )
    if formatted:
        formatted = f"""
<memories>
{formatted}
</memories>"""

    # Prepare the system prompt with user memories and current time
    # This helps the model understand the context and temporal relevance
    sys = system_prompt.format(user_info=formatted, time=datetime.now().isoformat())

    # Invoke the language model with the prepared prompt and tools
    # "bind_tools" gives the LLM the JSON schema for all tools in the list so it knows how
    # to use them.
    msg = await llm.bind_tools([tools.upsert_memory]).ainvoke(
        [{"role": "system", "content": sys}, *state.messages],
        context=utils.split_model_and_provider(model),
    )
    return {"messages": [msg]}


async def request_confirmation(state: State, runtime: Runtime[Context]):
    """Request user confirmation before saving memories."""
    # Extract tool calls from the last message
    tool_calls = getattr(state.messages[-1], "tool_calls", [])
    
    if not tool_calls:
        return {"messages": []}
    
    # Store pending memories for later confirmation
    pending_memories = []
    for tc in tool_calls:
        pending_memories.append({
            "tool_call_id": tc["id"],
            "args": tc["args"]
        })
    
    # Create confirmation message
    confirmation_msg = {
        "role": "assistant",
        "content": f"I'd like to save {len(pending_memories)} memory(ies). Type 'accept' to confirm, or anything else to reject."
    }
    
    return {
        "messages": [confirmation_msg],
        "pending_memories": pending_memories,
        "awaiting_confirmation": True
    }


async def process_confirmation(state: State, runtime: Runtime[Context]):
    """Process user's confirmation response."""
    if not state.awaiting_confirmation or not state.pending_memories:
        return {"awaiting_confirmation": False, "pending_memories": []}
    
    # Get the user's last message
    user_response = state.messages[-1].content.strip().lower()
    
    if user_response == "accept":
        # Save the memories
        saved_memories = await asyncio.gather(
            *(
                tools.upsert_memory(
                    **memory["args"],
                    user_id=runtime.context.user_id,
                    store=cast(BaseStore, runtime.store),
                )
                for memory in state.pending_memories
            )
        )
        
        # Format the results
        results = [
            {
                "role": "tool", 
                "content": mem,
                "tool_call_id": memory["tool_call_id"],
            }
            for memory, mem in zip(state.pending_memories, saved_memories)
        ]
        
        success_msg = {
            "role": "assistant",
            "content": f"âœ“ Successfully saved {len(saved_memories)} memory(ies)!"
        }
        
        return {
            "messages": results + [success_msg],
            "awaiting_confirmation": False,
            "pending_memories": []
        }
    else:
        # Reject the memories
        rejection_msg = {
            "role": "assistant", 
            "content": "Memory save rejected. No memories were stored."
        }
        
        return {
            "messages": [rejection_msg],
            "awaiting_confirmation": False,
            "pending_memories": []
        }


def route_message(state: State):
    """Determine the next step based on the state."""
    # If we're awaiting confirmation, process the user's response
    if state.awaiting_confirmation:
        return "process_confirmation"
    
    # Check if there are tool calls to process
    msg = state.messages[-1]
    if getattr(msg, "tool_calls", None):
        # If there are tool calls, request confirmation first
        return "request_confirmation"
    
    # Otherwise, finish; user can send the next message
    return END


# Create the graph + all nodes
builder = StateGraph(State, context_schema=Context)

# Define the flow of the memory extraction process
builder.add_node(call_model)
builder.add_edge("__start__", "call_model")

# Add confirmation nodes
builder.add_node(request_confirmation)
builder.add_node(process_confirmation)

# Define the routing logic
builder.add_conditional_edges("call_model", route_message, ["request_confirmation", END])
builder.add_edge("request_confirmation", END)  # Return control to user for confirmation
builder.add_edge("process_confirmation", "call_model")  # After confirmation, generate response

graph = builder.compile()
graph.name = "MemoryAgent"


__all__ = ["graph"]




File Name: prompts.py

-----------------------------

File Content: 

"""Define default prompts."""

SYSTEM_PROMPT = """You are a helpful and friendly chatbot. Get to know the user! \
Ask questions! Be spontaneous! 
{user_info}

System Time: {time}"""




File Name: state.py

-----------------------------

File Content: 

"""Define the shared values."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import Annotated


@dataclass(kw_only=True)
class State:
    """Main graph state."""

    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    
    pending_memories: Optional[list[dict]] = field(default=None)
    """Memories awaiting user confirmation."""
    
    awaiting_confirmation: bool = field(default=False)
    """Whether we're waiting for user confirmation to save memories."""


__all__ = [
    "State",
]




File Name: tools.py

-----------------------------

File Content: 

"""Define he agent's tools."""

import uuid
from typing import Annotated, Optional
from enum import Enum

from langchain_core.tools import InjectedToolArg
from langgraph.store.base import BaseStore


class MemoryCategory(str, Enum):
    """Memory categories for classification."""
    PERSONAL = "personal"
    PROFESSIONAL = "professional"  
    OTHER = "other"


async def upsert_memory(
    content: str,
    context: str,
    category: MemoryCategory,
    *,
    memory_id: Optional[uuid.UUID] = None,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Upsert a memory in the database.

    If a memory conflicts with an existing one, then just UPDATE the
    existing one by passing in memory_id - don't create two memories
    that are the same. If the user corrects a memory, UPDATE it.

    Args:
        content: The main content of the memory. For example:
            "User expressed interest in learning about French."
        context: Additional context for the memory. For example:
            "This was mentioned while discussing career options in Europe."
        category: The category of the memory - personal, professional, or other.
            Personal: family, friends, hobbies, personal interests, health, emotions
            Professional: work, career, skills, education, business contacts
            Other: general knowledge, preferences, miscellaneous facts
        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.
        The memory to overwrite.
    """
    mem_id = memory_id or uuid.uuid4()
    
    # Store with category-based namespace
    namespace = ("memories", user_id, category.value)
    await store.aput(
        namespace,
        key=str(mem_id),
        value={"content": content, "context": context, "category": category.value},
    )
    return f"Stored {category.value} memory {mem_id}"


async def classify_message_category(messages: list, llm) -> MemoryCategory:
    """Classify the category of messages for memory retrieval.
    
    Args:
        messages: List of recent messages to classify
        llm: Language model for classification
        
    Returns:
        MemoryCategory: The predicted category
    """
    classification_prompt = """
    Analyze these messages and determine if they are primarily about:
    - PERSONAL: family, friends, hobbies, personal interests, health, emotions, personal life
    - PROFESSIONAL: work, career, skills, education, business contacts, professional development  
    - OTHER: general knowledge, preferences, facts, or mixed topics

    Messages: {messages}
    
    Respond with only one word: PERSONAL, PROFESSIONAL, or OTHER
    """
    
    message_text = "\n".join([str(msg.content) if hasattr(msg, 'content') else str(msg) for msg in messages[-3:]])
    
    response = await llm.ainvoke([{
        "role": "user", 
        "content": classification_prompt.format(messages=message_text)
    }])
    
    category_text = response.content.strip().upper()
    
    if "PERSONAL" in category_text:
        return MemoryCategory.PERSONAL
    elif "PROFESSIONAL" in category_text:
        return MemoryCategory.PROFESSIONAL
    else:
        return MemoryCategory.OTHER




File Name: utils.py

-----------------------------

File Content: 

"""Utility functions used in our graph."""


def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


