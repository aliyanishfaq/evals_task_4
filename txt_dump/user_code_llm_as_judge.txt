File Name: context.py

-----------------------------

File Content: 

"""Define the runtime context information for the agent."""

import os
from dataclasses import dataclass, field, fields

from typing_extensions import Annotated

from . import prompts


@dataclass(kw_only=True)
class Context:
    """Main context class for the memory graph system."""

    user_id: str = "default"
    """The ID of the user to remember in the conversation."""

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="anthropic/claude-3-5-sonnet-20240620",
        metadata={
            "description": "The name of the language model to use for the agent. "
            "Should be in the form: provider/model-name."
        },
    )

    system_prompt: str = prompts.SYSTEM_PROMPT

    def __post_init__(self):
        """Fetch env vars for attributes that were not passed as args."""
        for f in fields(self):
            if not f.init:
                continue

            if getattr(self, f.name) == f.default:
                setattr(self, f.name, os.environ.get(f.name.upper(), f.default))




File Name: graph.py

-----------------------------

File Content: 

"""Graphs that extract memories on a schedule."""

import asyncio
import logging
from datetime import datetime
from typing import cast

from langchain.chat_models import init_chat_model
from langgraph.graph import END, StateGraph
from langgraph.runtime import Runtime
from langgraph.store.base import BaseStore
from langgraph.types import interrupt

from . import tools, utils
from .context import Context
from .state import State

logger = logging.getLogger(__name__)

# Initialize the language model to be used for memory extraction
llm = init_chat_model(model="claude-3-5-sonnet-20240620", model_provider="anthropic")


async def call_model(state: State, runtime: Runtime[Context]) -> dict:
    """Extract the user's state from the conversation and update the memory."""
    user_id = runtime.context.user_id
    model = runtime.context.model
    system_prompt = runtime.context.system_prompt

    # Determine relevant categories for the current conversation
    relevant_categories = await tools.get_message_categories(
        state.messages, user_id=user_id
    )
    
    # Retrieve memories from relevant categories only
    all_memories = []
    for category in relevant_categories:
        memories = await cast(BaseStore, runtime.store).asearch(
            ("memories", user_id, category.value),
            query=str([m.content for m in state.messages[-3:]]),
            limit=5,  # Reduced per category to avoid overwhelming context
        )
        all_memories.extend(memories)

    # Format memories for inclusion in the prompt
    formatted = "\n".join(
        f"[{mem.key}]: {mem.value} (similarity: {mem.score})" for mem in all_memories
    )
    if formatted:
        formatted = f"""
<memories>
{formatted}
</memories>"""

    # Prepare the system prompt with user memories and current time
    # This helps the model understand the context and temporal relevance
    sys = system_prompt.format(user_info=formatted, time=datetime.now().isoformat())

    # Invoke the language model with the prepared prompt and tools
    # "bind_tools" gives the LLM the JSON schema for all tools in the list so it knows how
    # to use them.
    msg = await llm.bind_tools([tools.upsert_memory]).ainvoke(
        [{"role": "system", "content": sys}, *state.messages],
        context=utils.split_model_and_provider(model),
    )
    return {"messages": [msg]}


async def prepare_memory_confirmation(state: State, runtime: Runtime[Context]):
    """Prepare memories for user confirmation."""
    # Extract tool calls from the last message
    tool_calls = getattr(state.messages[-1], "tool_calls", [])
    
    if not tool_calls:
        return {"pending_memories": None}
    
    # Prepare memory details for confirmation
    pending_memories = []
    for tc in tool_calls:
        memory_info = {
            "tool_call_id": tc["id"],
            "content": tc["args"].get("content", ""),
            "context": tc["args"].get("context", ""),
            "memory_id": tc["args"].get("memory_id"),
        }
        
        # Categorize the memory
        category = await tools.categorize_memory(
            memory_info["content"],
            memory_info["context"], 
            user_id=runtime.context.user_id
        )
        memory_info["category"] = category.value
        pending_memories.append(memory_info)
    
    # Format confirmation message
    memory_summaries = []
    for i, mem in enumerate(pending_memories, 1):
        memory_summaries.append(
            f"{i}. [{mem['category'].upper()}] {mem['content']}\n   Context: {mem['context']}"
        )
    
    confirmation_text = f"""I'd like to save the following memor{'ies' if len(pending_memories) > 1 else 'y'}:

{chr(10).join(memory_summaries)}

Type 'accept' to save {'these memories' if len(pending_memories) > 1 else 'this memory'}, or anything else to reject."""
    
    # Use interrupt to ask for user confirmation
    user_response = interrupt(confirmation_text)
    
    return {
        "pending_memories": pending_memories,
        "user_confirmation": user_response
    }


async def store_memory(state: State, runtime: Runtime[Context]):
    """Store memories based on user confirmation."""
    if not state.pending_memories:
        return {"messages": []}
    
    user_response = state.user_confirmation
    
    if user_response and user_response.lower().strip() == "accept":
        # User accepted, store the memories
        saved_memories = []
        for memory_info in state.pending_memories:
            # Store using the already categorized information
            mem_id = memory_info["memory_id"] or tools.uuid.uuid4()
            
            await cast(BaseStore, runtime.store).aput(
                ("memories", runtime.context.user_id, memory_info["category"]),
                key=str(mem_id),
                value={
                    "content": memory_info["content"], 
                    "context": memory_info["context"],
                    "category": memory_info["category"]
                },
            )
            saved_memories.append(f"Stored {memory_info['category']} memory {mem_id}")
        
        # Format the results
        results = [
            {
                "role": "tool",
                "content": saved_mem,
                "tool_call_id": memory_info["tool_call_id"],
            }
            for memory_info, saved_mem in zip(state.pending_memories, saved_memories)
        ]
        
        return {
            "messages": results,
            "pending_memories": None,
            "user_confirmation": None
        }
    else:
        # User rejected, create rejection messages
        results = [
            {
                "role": "tool", 
                "content": "Memory storage rejected by user",
                "tool_call_id": memory_info["tool_call_id"],
            }
            for memory_info in state.pending_memories
        ]
        
        return {
            "messages": results,
            "pending_memories": None,
            "user_confirmation": None
        }


def route_after_model(state: State):
    """Determine the next step after model response."""
    msg = state.messages[-1]
    if getattr(msg, "tool_calls", None):
        # If there are tool calls, prepare for user confirmation
        return "prepare_confirmation"
    # Otherwise, finish; user can send the next message
    return END


def route_after_confirmation(state: State):
    """Route after user confirmation is prepared."""
    if state.pending_memories:
        return "store_memory"
    return END


# Create the graph + all nodes
builder = StateGraph(State, context_schema=Context)

# Define the flow of the enhanced memory extraction process
builder.add_node("call_model", call_model)
builder.add_edge("__start__", "call_model")

builder.add_node("prepare_confirmation", prepare_memory_confirmation)
builder.add_node("store_memory", store_memory)

# Route from model to either confirmation or end
builder.add_conditional_edges(
    "call_model", 
    route_after_model, 
    ["prepare_confirmation", END]
)

# After confirmation, always go to store_memory
builder.add_edge("prepare_confirmation", "store_memory")

# After storing memory, return to model to generate response
builder.add_edge("store_memory", "call_model")

graph = builder.compile()
graph.name = "MemoryAgent"


__all__ = ["graph"]




File Name: prompts.py

-----------------------------

File Content: 

"""Define default prompts."""

SYSTEM_PROMPT = """You are a helpful and friendly chatbot. Get to know the user! \
Ask questions! Be spontaneous! 
{user_info}

System Time: {time}"""




File Name: state.py

-----------------------------

File Content: 

"""Define the shared values."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Any

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import Annotated


@dataclass(kw_only=True)
class State:
    """Main graph state."""

    messages: Annotated[list[AnyMessage], add_messages]
    """The messages in the conversation."""
    
    pending_memories: Optional[list[dict[str, Any]]] = None
    """Memories waiting for user confirmation."""
    
    user_confirmation: Optional[str] = None
    """User's response to memory confirmation prompt."""


__all__ = [
    "State",
]




File Name: tools.py

-----------------------------

File Content: 

"""Define he agent's tools."""

import uuid
from typing import Annotated, Optional
from enum import Enum
from pydantic import BaseModel

from langchain_core.tools import InjectedToolArg
from langgraph.store.base import BaseStore
from langchain.chat_models import init_chat_model


class MemoryCategory(str, Enum):
    """Categories for memory classification."""
    PERSONAL = "personal"
    PROFESSIONAL = "professional"  
    OTHER = "other"


class CategoryClassification(BaseModel):
    """Schema for memory category classification."""
    category: MemoryCategory
    reasoning: str


async def categorize_memory(
    content: str,
    context: str,
    *,
    user_id: Annotated[str, InjectedToolArg],
) -> MemoryCategory:
    """Categorize a memory into personal, professional, or other."""
    llm = init_chat_model(model="claude-3-5-sonnet-20240620", model_provider="anthropic")
    
    categorization_prompt = f"""
    Analyze the following memory and categorize it as either 'personal', 'professional', or 'other'.

    Memory Content: {content}
    Memory Context: {context}

    Guidelines:
    - Personal: Family, friends, hobbies, personal interests, health, relationships, personal goals
    - Professional: Work, career, business, colleagues, professional skills, job-related information
    - Other: General knowledge, neutral information that doesn't clearly fit personal or professional categories

    Return your classification with reasoning.
    """
    
    result = await llm.with_structured_output(CategoryClassification).ainvoke([
        {"role": "user", "content": categorization_prompt}
    ])
    
    return result.category


async def upsert_memory(
    content: str,
    context: str,
    *,
    memory_id: Optional[uuid.UUID] = None,
    # Hide these arguments from the model.
    user_id: Annotated[str, InjectedToolArg],
    store: Annotated[BaseStore, InjectedToolArg],
):
    """Upsert a memory in the database with automatic categorization.

    If a memory conflicts with an existing one, then just UPDATE the
    existing one by passing in memory_id - don't create two memories
    that are the same. If the user corrects a memory, UPDATE it.

    Args:
        content: The main content of the memory. For example:
            "User expressed interest in learning about French."
        context: Additional context for the memory. For example:
            "This was mentioned while discussing career options in Europe."
        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.
        The memory to overwrite.
    """
    # Categorize the memory
    category = await categorize_memory(content, context, user_id=user_id)
    
    mem_id = memory_id or uuid.uuid4()
    
    # Store memory with category in the namespace
    await store.aput(
        ("memories", user_id, category.value),
        key=str(mem_id),
        value={"content": content, "context": context, "category": category.value},
    )
    return f"Stored {category.value} memory {mem_id}"


async def get_message_categories(
    messages: list,
    *,
    user_id: Annotated[str, InjectedToolArg],
) -> list[MemoryCategory]:
    """Determine the categories relevant to recent messages."""
    llm = init_chat_model(model="claude-3-5-sonnet-20240620", model_provider="anthropic")
    
    # Get the last few messages for context
    recent_messages = messages[-3:] if len(messages) >= 3 else messages
    message_content = "\n".join([f"- {msg.content}" for msg in recent_messages if hasattr(msg, 'content')])
    
    categorization_prompt = f"""
    Analyze the following conversation messages and determine which memory categories are most relevant:
    
    Recent Messages:
    {message_content}
    
    Available categories: personal, professional, other
    
    Return the categories that are most relevant to this conversation context.
    You can return multiple categories if the conversation spans different areas.
    """
    
    class RelevantCategories(BaseModel):
        categories: list[MemoryCategory]
        reasoning: str
    
    result = await llm.with_structured_output(RelevantCategories).ainvoke([
        {"role": "user", "content": categorization_prompt}
    ])
    
    return result.categories




File Name: utils.py

-----------------------------

File Content: 

"""Utility functions used in our graph."""


def split_model_and_provider(fully_specified_name: str) -> dict:
    """Initialize the configured chat model."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = None
        model = fully_specified_name
    return {"model": model, "provider": provider}


